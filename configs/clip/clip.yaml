model:
  base_learning_rate: 2e-5
  target: ldm.models.diffusion.clip.MaskTextClipModule
  params:
    image_key: mask
    text_key: context
    use_scheduler: true 
    scheduler_config:
      target: torch.optim.lr_scheduler.LinearLR
      params:
        start_factor: 1
        end_factor: 1e-3
        total_iters: 500

    image_encoder_config:
      target: ldm.modules.vision_mamba.Vim
      params:
        dim: 128              # Dimension of the transformer model
        dt_rank: 32           # Rank of the dynamic routing matrix
        dim_inner: 128        # Inner dimension of the transformer model
        d_state: 256          # Dimension of the state vector
        num_classes: 1024     # Number of output classes
        image_size: [64, 128, 128]  # Size of the input image
        patch_size: 16        # Size of each image patch
        channels: 1           # Number of input channels
        dropout: 0.1          # Dropout rate
        depth: 12             # Depth of the transformer model
        spatial_dims: 3

    text_encoder_config: 
      target: torch.nn.Identity

    text_proj_in: 20480       # 5120 * 4

data:
  target: main.DataModuleFromConfig
  params:
    batch_size: 8
    num_workers: 16
    wrap: false
    train:
      target: ldm.data.ruijin.Ruijin_3D
      params:
        split: train
        resize_to: [64, 128, 128]
        context_len: 4
    validation:
      target: ldm.data.ruijin.Ruijin_3D
      params:
        split: val
        resize_to: [64, 128, 128]
        context_len: 4

lightning:
  callbacks:
    image_logger:
      target: main.ImageLogger
      params:
        train_batch_frequency: 200
        val_batch_frequency: 100
        max_images: 10

  trainer:
    benchmark: True
    max_epochs: 1000